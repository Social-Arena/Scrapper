# Scrapper - Social Media Data Collection Engine

A comprehensive data collection engine for gathering and processing social media content from multiple platforms. Part of a larger social media viral propagation agent simulation system.

## Overview

Scrapper is responsible for:
- **Multi-platform data collection**: Twitter, TikTok, XiaoHongShu (å°çº¢ä¹¦), YouTube, and more
- **Real-time trend monitoring**: Detecting and tracking viral content and trending topics
- **Data normalization**: Converting platform-specific data into a unified Feed format
- **Data enrichment**: Adding sentiment analysis, entity recognition, and topic classification

## Project Structure

```
Scrapper/
â”œâ”€â”€ scrapper/                 # Main package
â”‚   â”œâ”€â”€ sources/              # Platform-specific scrapers
â”‚   â”‚   â”œâ”€â”€ twitter_scraper.py
â”‚   â”‚   â”œâ”€â”€ tiktok_scraper.py
â”‚   â”‚   â”œâ”€â”€ xiaohongshu_scraper.py
â”‚   â”‚   â””â”€â”€ youtube_scraper.py
â”‚   â”œâ”€â”€ data_processing/      # Data processing pipeline
â”‚   â”‚   â”œâ”€â”€ content_normalizer.py
â”‚   â”‚   â”œâ”€â”€ trend_detector.py
â”‚   â”‚   â””â”€â”€ sentiment_analyzer.py
â”‚   â”œâ”€â”€ feeds/                # Feed management
â”‚   â”‚   â”œâ”€â”€ feed_aggregator.py
â”‚   â”‚   â””â”€â”€ feed_enricher.py
â”‚   â”œâ”€â”€ storage/              # Data storage
â”‚   â”‚   â”œâ”€â”€ raw_data_store.py
â”‚   â”‚   â””â”€â”€ cache_manager.py
â”‚   â”œâ”€â”€ monitoring/           # System monitoring
â”‚   â”‚   â”œâ”€â”€ scraping_monitor.py
â”‚   â”‚   â””â”€â”€ rate_limiter.py
â”‚   â”œâ”€â”€ config/               # Configuration
â”‚   â”‚   â””â”€â”€ logging_config.py
â”‚   â””â”€â”€ utils/                # Utilities
â”‚       â””â”€â”€ logger.py         # Centralized logging
â”œâ”€â”€ trace/                    # Runtime logs (git-ignored)
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ feeds/
â”‚   â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ errors/
â”‚   â”œâ”€â”€ performance/
â”‚   â””â”€â”€ README.md            # Logging documentation
â”œâ”€â”€ test_logging.py          # Logging system test
â”œâ”€â”€ LOGGING_USAGE.md         # Logging usage guide
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md               # This file
```

## Features

### âœ… Implemented

#### Comprehensive Logging System
- **File-based logging** (NO console output)
- **Component-specific logs** for easy debugging
- **Automatic log rotation** to prevent disk space issues
- **Performance tracking** for all operations
- **Structured logging** with JSON support
- **Error tracking** with full tracebacks and context

See [LOGGING_USAGE.md](LOGGING_USAGE.md) for detailed usage instructions.

### ğŸš§ Planned Features

#### Data Collection
- [ ] Twitter scraper with trending topics, hashtags, and user timelines
- [ ] TikTok scraper for viral videos and trends
- [ ] XiaoHongShu (å°çº¢ä¹¦) scraper for lifestyle content
- [ ] YouTube scraper for trending videos and channels
- [ ] Real-time content streaming
- [ ] Rate limiting and API quota management

#### Data Processing
- [ ] Content normalization to unified Feed format
- [ ] Trend detection and lifecycle analysis
- [ ] Sentiment analysis and emotional triggers
- [ ] Entity recognition and topic classification
- [ ] Virality prediction algorithms

#### Storage & Caching
- [ ] PostgreSQL for persistent storage
- [ ] Redis for caching and rate limiting
- [ ] Data deduplication
- [ ] Automatic cleanup of old data

#### Monitoring & Health
- [ ] Scraping health monitoring
- [ ] Data quality tracking
- [ ] Anomaly detection
- [ ] Performance metrics and benchmarks

## Getting Started

### Prerequisites

- Python 3.9 or higher
- (Additional dependencies to be added as features are implemented)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd Scrapper

# Install dependencies
pip install -r requirements.txt

# Test the logging system
python test_logging.py
```

### Quick Start

```python
from scrapper.utils.logger import initialize_logging, get_logger

# Initialize logging system
initialize_logging(log_level="INFO")

# Get a logger for your component
logger = get_logger("MyComponent")

# Start logging
logger.info("Application started")
```

## Logging System

### NO Console Output

All logs are written to files in the `trace/` directory. This ensures:
- Complete debugging capability after the fact
- No log loss due to terminal closure
- Easy log analysis with standard tools
- Structured logging for machine parsing

### Log Files

```
trace/
â”œâ”€â”€ scrapers/          # Platform scraper logs
â”œâ”€â”€ processing/        # Data processing logs
â”œâ”€â”€ feeds/             # Feed system logs
â”œâ”€â”€ storage/           # Storage operation logs
â”œâ”€â”€ monitoring/        # Monitoring and health logs
â”œâ”€â”€ errors/            # Error and exception logs
â”œâ”€â”€ performance/       # Performance metrics
â””â”€â”€ main.log          # Main application log
```

### Usage Example

```python
from scrapper.utils.logger import get_logger, log_performance

logger = get_logger("TwitterScraper")

# Log messages
logger.info("Starting to scrape trending topics")
logger.debug(f"API request to endpoint: {endpoint}")
logger.warning("Rate limit approaching")

# Log performance
with log_performance(logger, "scrape_trending_topics"):
    topics = scrape_trending_topics()

# Log errors with context
try:
    result = risky_operation()
except Exception as e:
    log_error_with_context(logger, e, {"user_id": user_id})
```

### Debugging Workflow

1. **Identify the component** involved in the issue
2. **Check error logs** first: `trace/errors/errors.log`
3. **Follow the execution flow** across component logs
4. **Check performance** if needed: `trace/performance/metrics.log`
5. **Analyze with standard tools**: `grep`, `tail`, `awk`, etc.

See [trace/README.md](trace/README.md) for detailed debugging instructions.

## Development Guidelines

### Adding New Components

1. Create your component in the appropriate directory
2. Get a logger for your component:
   ```python
   from scrapper.utils.logger import get_logger

   class MyComponent:
       def __init__(self):
           self.logger = get_logger("MyComponent")
   ```

3. Log at appropriate levels:
   - `DEBUG`: Detailed diagnostic information
   - `INFO`: General flow and operations
   - `WARNING`: Concerning but not critical issues
   - `ERROR`: Errors that need attention
   - `CRITICAL`: System-threatening errors

4. Include context in logs:
   ```python
   logger.info("Operation completed", extra={"extra_data": {
       "items_processed": count,
       "duration": elapsed,
       "platform": platform
   }})
   ```

### Testing

Always test your logging:

```bash
# Run the logging test
python test_logging.py

# Check that logs are created
ls -lh trace/**/*.log

# Verify log content
tail -50 trace/main.log
```

## Compliance and Ethics

- Respects platform API terms of service
- Implements rate limiting to avoid abuse
- Does not collect personal identifiable information
- Supports data deletion requests
- Includes data anonymization features

## Architecture

### Data Flow

```
[Platform APIs]
      â†“
[Platform Scrapers] â†’ logs to trace/scrapers/
      â†“
[Content Normalizer] â†’ logs to trace/processing/
      â†“
[Feed Enricher] â†’ logs to trace/feeds/
      â†“
[Storage Layer] â†’ logs to trace/storage/
      â†“
[Feed System / Arena System]
```

### Key Components

- **Scrapers**: Platform-specific data collection
- **Normalizers**: Convert to unified format
- **Enrichers**: Add metadata and analysis
- **Storage**: Persist and cache data
- **Monitors**: Track health and performance

## Performance Considerations

- Async/await for concurrent operations
- Connection pooling for database access
- Redis caching for frequently accessed data
- Rate limiting to respect API quotas
- Automatic log rotation to manage disk space

## Contributing

1. Follow the logging guidelines
2. Add appropriate tests
3. Document new features
4. Ensure logs are helpful for debugging

## Roadmap

### Phase 1: Foundation (Current)
- [x] Logging infrastructure
- [ ] Twitter scraper
- [ ] Data normalization pipeline
- [ ] Basic storage

### Phase 2: Expansion
- [ ] Additional platform scrapers
- [ ] Trend detection
- [ ] Sentiment analysis
- [ ] Performance optimization

### Phase 3: Advanced Features
- [ ] Real-time streaming
- [ ] Advanced analytics
- [ ] Machine learning integration
- [ ] Distributed scraping

## Support

- **Documentation**: See [LOGGING_USAGE.md](LOGGING_USAGE.md)
- **Debugging**: See [trace/README.md](trace/README.md)
- **Issues**: Check logs in `trace/` directory

## License

See [LICENSE](LICENSE) file for details.

---

**Note**: This project is under active development. The logging system is fully functional. Other features are being implemented according to the roadmap
